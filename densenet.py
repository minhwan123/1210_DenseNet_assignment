# -*- coding: utf-8 -*-
"""DenseNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ePVa_86-CHNduJLdJY_JfTRvv_xPNfR
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision tqdm scikit-learn

import os
import copy

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms, models

from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DEVICE

DATA_ROOT = "/content/drive/MyDrive/POC_Dataset"
TRAIN_DIR = os.path.join(DATA_ROOT, "Training")
TEST_DIR  = os.path.join(DATA_ROOT, "Testing")

NUM_CLASSES = 4                    # Chorionic, Decidual, Hemorrhage, Trophoblastic
BATCH_SIZE = 32
NUM_EPOCHS = 50
LR = 1e-4
VAL_RATIO = 0.2                    # Training ì¼ë¶€ë¥¼ validationìœ¼ë¡œ ì‚¬ìš©
IMG_SIZE = 224
SAVE_PATH = "best_densenet.pth"

print("Train dir:", TRAIN_DIR)
print("Test  dir:", TEST_DIR)
print("Device  :", DEVICE)

def get_dataloaders():
    # í•™ìŠµìš© ë°ì´í„°ì—ëŠ” augmentation ì ìš©
    train_transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])

    # ê²€ì¦/í…ŒìŠ¤íŠ¸ëŠ” augmentation ì—†ì´
    val_test_transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])

    full_train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)
    test_dataset       = datasets.ImageFolder(TEST_DIR,  transform=val_test_transform)

    class_names = full_train_dataset.classes
    print("Classes:", class_names)

    # train/val split
    n_total = len(full_train_dataset)
    n_val = int(n_total * VAL_RATIO)
    n_train = n_total - n_val

    train_dataset, val_dataset = random_split(full_train_dataset, [n_train, n_val])

    print(f"Train samples: {len(train_dataset)}")
    print(f"Val samples:   {len(val_dataset)}")
    print(f"Test samples:  {len(test_dataset)}")

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                              shuffle=True, num_workers=4, pin_memory=True)
    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE,
                              shuffle=False, num_workers=4, pin_memory=True)
    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE,
                              shuffle=False, num_workers=4, pin_memory=True)

    return train_loader, val_loader, test_loader, class_names

train_loader, val_loader, test_loader, class_names = get_dataloaders()

def build_densenet(num_classes: int) -> nn.Module:
    # torch ë²„ì „ì— ë”°ë¼ weights ì¸ì ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ try/except
    try:
        densenet = models.densenet121(
            weights=models.DenseNet121_Weights.IMAGENET1K_V1
        )
    except Exception:
        # êµ¬ë²„ì „(torch<1.13 ë“±)ì¼ ê²½ìš°
        densenet = models.densenet121(pretrained=True)

    # ë§ˆì§€ë§‰ classifier êµì²´
    in_features = densenet.classifier.in_features
    densenet.classifier = nn.Linear(in_features, num_classes)
    return densenet

model = build_densenet(NUM_CLASSES).to(DEVICE)
model

def train_one_epoch(model, loader, criterion, optimizer, epoch):
    model.train()
    running_loss = 0.0
    running_corrects = 0
    total = 0

    pbar = tqdm(loader, desc=f"Epoch {epoch} [Train]")
    for inputs, labels in pbar:
        inputs = inputs.to(DEVICE)
        labels = labels.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, preds = torch.max(outputs, 1)

        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)
        total += inputs.size(0)

        pbar.set_postfix({
            "loss": f"{running_loss / total:.4f}",
            "acc":  f"{running_corrects.double().item() / total:.4f}"
        })

    epoch_loss = running_loss / total
    epoch_acc  = running_corrects.double().item() / total
    return epoch_loss, epoch_acc


def eval_model(model, loader, criterion, epoch, phase="Val"):
    model.eval()
    running_loss = 0.0
    running_corrects = 0
    total = 0

    all_labels = []
    all_preds = []

    with torch.no_grad():
        pbar = tqdm(loader, desc=f"Epoch {epoch} [{phase}]")
        for inputs, labels in pbar:
            inputs = inputs.to(DEVICE)
            labels = labels.to(DEVICE)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            _, preds = torch.max(outputs, 1)

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
            total += inputs.size(0)

            all_labels.extend(labels.cpu().numpy().tolist())
            all_preds.extend(preds.cpu().numpy().tolist())

            pbar.set_postfix({
                "loss": f"{running_loss / total:.4f}",
                "acc":  f"{running_corrects.double().item() / total:.4f}"
            })

    epoch_loss = running_loss / total
    epoch_acc  = running_corrects.double().item() / total
    return epoch_loss, epoch_acc, all_labels, all_preds

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)

best_val_acc = 0.0
best_model_wts = copy.deepcopy(model.state_dict())

for epoch in range(1, NUM_EPOCHS + 1):
    train_loss, train_acc = train_one_epoch(
        model, train_loader, criterion, optimizer, epoch
    )

    val_loss, val_acc, _, _ = eval_model(
        model, val_loader, criterion, epoch, phase="Val"
    )

    print(f"\nEpoch {epoch} Summary")
    print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
    print(f"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}")

    if val_acc > best_val_acc:
        print(f"ğŸ”¹ New best model! (val_acc: {best_val_acc:.4f} â†’ {val_acc:.4f})")
        best_val_acc = val_acc
        best_model_wts = copy.deepcopy(model.state_dict())
        torch.save(best_model_wts, SAVE_PATH)

print(f"\nâœ… Training finished. Best Val Acc: {best_val_acc:.4f}")
print(f"Best model saved to: {SAVE_PATH}")

# ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ê°€ì¤‘ì¹˜ ë¡œë“œ
model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))

test_loss, test_acc, y_true, y_pred = eval_model(
    model, test_loader, criterion, epoch="Test", phase="Test"
)

print(f"\nğŸ¯ Test Accuracy: {test_acc:.4f}\n")

print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred))